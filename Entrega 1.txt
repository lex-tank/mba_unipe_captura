GRUPO

	ALEXANDRE SILVA
	ANÍBAL MEDEIROS


3)Quais cuidados devem ser observados ao capturar dados de um site?

*Verificar se o conteúdo capturado não tem direitos autorais 
	- Caso tenha, o scraper pode ser acusado pelo código penal.

*Se o scraping não onera o serviços do site
	- Pode ocasionar lentidão e derrubada dos servidores, podendo causar prejuízos consideráveis tanto aos demais usuários quanto a empresa, ocasionando prejuízos financeiros.

*Se o scraper não viola os termos de uso do site
	- Violar os termos de uso pode gerar de um simples banimento de uma conta, até a responsabilização civil e/ou criminal. 

*Se o scraper não coleta informações sensíveis
	- A LGPD trata os dados sensíveis no inciso II, do art. 5º como sendo "dado pessoal sobre origem racial ou étnica, convicção religiosa, opinião política, filiação a sindicato ou a organização de caráter religioso, filosófico ou político, dado referente à saúde ou à vida sexual, dado genético ou biométrico, quando vinculado a uma pessoa natural". Violar esses dados pode causar a discriminação de uma pessoa. Além desses tipos de dados, podemos considerar informações sigilosas de empresas que possam favorecer terceiros na bolsa de valores por exemplo. Há uma infinidade de situações que podem se encaixar como informação sensível.


4)Quais ameaças capturas automáticas proporcionam para sistemas web?

Questão respondida no exercício 3. Para complementar, é necessário tomar cuidado ao executar web scraping. Dados estão cada vez mais protegidos e capturar informações pode não somente prejudicar empresas financeiramente, mas também o fluxo de négocios ao interromper o funcionamento por excesso de requests no servidor. É preciso responsabilidade sobre o que se extrai, pois se for informação sigilosa ou sensível, ou até mesmo dados expostos que não deveriam ser vazados, o responsável pode responder civil ou criminalmente, além de reparos de danos financeiros que possam ser causados.


5)Você diria que bots ou crawlers são programas facilmente paralelizáveis? Se sim, explique como isso seria implementado dando um exemplo. 

Como foi visto em aula, tais processos podem ser facilmente paralelizados, pois não há necessidade de ordem na execução nem troca de informações entre as tarefas.


Pesquisando exemplo na net, achei um código que utiliza threads para paralelizar pesquisa de 40 mil links obtidos de um arquivo CSV. Segue o trecho do código usando thread.

if __name__ == "__main__":
     #le data frame
    links = pd.read_csv('offers.csv', names=['Link'])
    links = links['Link'].values.tolist()

    crawler = Crawler()
    t1 = threading.Thread(target=crawler.start, args=(links[0:10000],)) 
    t2 = threading.Thread(target=crawler.start, args=(links[10001:20000],)) 
    t3 = threading.Thread(target=crawler.start, args=(links[20001:30000],)) 
    t4 = threading.Thread(target=crawler.start, args=(links[30001:40000],)) 
   
    t1.start() 
    t2.start()
    t3.start()
    t4.start() 

    t1.join()
    t2.join()
    t3.join()
    t4.join()

    crawler.df.to_csv('dados.csv', index=False)
    crawler.insert_db()

    print("Scrapy feito!")
